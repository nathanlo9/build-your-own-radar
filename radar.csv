name,ring,quadrant,isNew,status,description
Data product thinking,Adopt,Techniques,FALSE,moved in,"<p>Organizations actively adopt <strong><a href=""https://www.thoughtworks.com/insights/e-books/modern-data-engineering-playbook/data-as-a-product"">data product thinking</a></strong> as a standard practice for managing data assets. This approach treats data as a product with its own lifecycle, quality standards and focus on meeting consumer needs. We now recommend it as default advice for data management, regardless of whether organizations choose architectures like <a href=""/radar/techniques/data-mesh"">data mesh</a> or <a href=""/radar/techniques/lakehouse-architecture"">lakehouse</a>.</p>

<p>We emphasize consumer-centricity in data product thinking to drive greater adoption and value realization. This means <a href=""https://martinfowler.com/articles/designing-data-products.html"">designing data products</a> by working backward from use cases. We also focus on capturing and managing both business-relevant metadata and technical metadata using modern data catalogs like <a href=""/radar/platforms/datahub"">DataHub</a>, <a href=""/radar/platforms/collibra"">Collibra</a>, Atlan and Informatica. These practices improve data discoverability and usability. Additionally, we apply data product thinking to scale AI initiatives and create AI-ready data. This approach includes comprehensive lifecycle management, ensuring data is not only well-governed and high quality but also retired in compliance with legal and regulatory requirements when no longer needed.</p>"
Fuzz testing,Adopt,Techniques,TRUE,new,"<p><strong>Fuzz testing</strong> , or simply fuzzing, is a testing technique that has been around for a long time but it is still one of the lesser-known techniques. The goal is to feed a software system all kinds of invalid input and observe its behavior. For an HTTP endpoint, for example, bad requests should result in 4<em>xx</em> errors, but fuzz testing often provokes 5<em>xx</em> errors or worse. Well-documented and supported by tools, fuzz testing is more relevant than ever with more AI-generated code and <a href=""/radar/techniques/complacency-with-ai-generated-code"">complacency with AI generated code</a>. That means it’s now a good time to adopt fuzz testing to ensure code remains robust and secure.</p>"
Software Bill of Materials,Adopt,Techniques,FALSE,moved in,"<p>Since our initial blip in 2021, <strong>Software Bill of Materials</strong> (SBOM) generation has transitioned from an emerging practice to a sensible default across our projects. The ecosystem has matured significantly, offering a robust tooling ecosystem and seamless CI/CD integration. Tools like <a href=""/radar/tools/syft"">Syft</a>, <a href=""/radar/tools/trivy"">Trivy</a> and <a href=""/radar/tools/snyk"">Snyk</a> provide comprehensive SBOM generation from source code to container images and vulnerability scanning. Platforms such as <a href=""/radar/tools/fossa"">FOSSA</a> and <a href=""/radar/platforms/chainloop"">Chainloop</a> enhance security risk management by integrating with development workflows and enforcing security policies. While a universal SBOM standard is still evolving, broad support for <a href=""https://spdx.dev"">SPDX</a> and <a href=""/radar/platforms/cyclonedx"">CycloneDX</a> has minimized adoption challenges. AI systems also require SBOMs, as evidenced by the UK Government's <a href=""https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice"">AI Cyber Security Code of Practice</a> and CISA's <a href=""https://www.cisa.gov/resources-tools/resources/ai-cybersecurity-collaboration-playbook"">AI Cybersecurity Collaboration Playbook</a>. We’ll continue to monitor developments in this space.</p>"
Threat modeling,Adopt,Techniques,FALSE,no change,"<p>In the rapidly evolving AI-driven landscape of software development, <strong><a href=""https://martinfowler.com/articles/agile-threat-modelling.html"">threat modeling</a></strong> is more crucial than ever for building secure software while maintaining agility and avoiding the <a href=""/radar/techniques/security-sandwich"">security sandwich</a>. Threat modeling — a set of techniques for identifying and classifying potential threats — applies across various contexts, <a href=""https://aws.amazon.com/blogs/security/threat-modeling-your-generative-ai-workload-to-evaluate-security-risk/"">including generative AI applications</a>, which introduce <a href=""https://nazneen-rupawalla.gitbook.io/learn-write-share/security-bits/threat-modelling-for-ai-changing-the-way-you-view-trust-boundaries"">unique security risks</a>. To be effective, it must be performed regularly throughout the software lifecycle and works best alongside other security practices. These include defining cross-functional security requirements to address common risks in the project's technologies and leveraging automated security scanners for continuous monitoring.</p>"
API request collection as API product artifact,Contain,Techniques,TRUE,new,"<p>Treating <a href=""/radar/techniques/apis-as-a-product"">APIs as a product</a> means prioritizing developer experience, not only by putting sensible and standard design into APIs themselves but also providing comprehensive documentation as well as smooth onboarding experiences. While OpenAPI (<a href=""/radar/tools/swagger"">Swagger</a>) specifications can effectively document API interfaces, onboarding remains a challenge. Developers need rapid access to working examples, with preconfigured authentication and realistic test data. With the maturing of API client tools (such as <a href=""/radar/tools/postman"">Postman</a>, <a href=""/radar/tools/bruno"">Bruno</a> and <a href=""/radar/tools/insomnia"">Insomnia</a>), we recommend treating <strong>API request collection as API product artifact</strong>. API request collections should be thoughtfully designed to guide developers through key workflows, helping them to understand the API's domain language and functionality with minimal effort. To keep collections up to date, we recommend storing them in a repository and integrating them into the API's release pipeline.</p>"
Architecture advice process,Contain,Techniques,TRUE,new,"<p>One of the persistent challenges in large software teams is determining who makes the architectural decisions that shape the evolution of systems. The <a href=""https://dora.dev/"">State of DevOps report</a> reveals that the traditional approach of Architecture Review Boards is counterproductive, often hindering workflow and correlating with low organizational performance. A compelling alternative is an <strong><a href=""https://martinfowler.com/articles/scaling-architecture-conversationally.html"">architectural advice process</a></strong> — a decentralized approach where anyone can make any architectural decision, provided they seek advice from those affected and those with relevant expertise. This method enables teams to optimize for flow without compromising architectural quality, both at small and large scales. At first glance, this approach may seem controversial, but practices such as <a href=""/radar/techniques/lightweight-architecture-decision-records"">Architecture Decision Records</a> and advisory forums ensure that decisions remain informed, while empowering those closest to the work to make decisions. We’ve seen this model succeed at scale in an increasing number of organizations, including those in highly regulated industries.</p>"
GraphRAG,Contain,Techniques,TRUE,new,"<p>In our last update of <a href=""/radar/techniques/retrieval-augmented-generation-rag"">RAG</a>, we introduced <strong>GraphRAG</strong> , originally described in <a href=""https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/"">Microsoft's article</a> as a two-step approach: (1) Chunking documents and using an LLM-based analysis of the chunks to create a knowledge graph; (2) retrieving relevant chunks at query time via embeddings while following edges in the knowledge graph to discover additional related chunks, which are then added to the augmented prompt. In many cases this approach enhances LLM-generated responses. We've observed similar benefits when <a href=""/radar/techniques/using-genai-to-understand-legacy-codebases"">using GenAI to understand legacy codebases</a>, where we use structural information — such as abstract syntax trees and dependencies — to build the knowledge graph. The GraphRAG pattern has gained traction, with tools and frameworks like Neo4j's <a href=""https://neo4j.com/blog/news/graphrag-python-package/"">GraphRAG Python package</a> emerging to support it. We also see <a href=""/radar/platforms/graphiti"">Graphiti</a> as fitting a broader interpretation of GraphRAG as a pattern.</p>"
Just-in-time privileged access management,Contain,Techniques,TRUE,new,"<p><a href=""https://csrc.nist.gov/glossary/term/least_privilege"">Least privilege</a> ensures users and systems have only the minimum access required to perform their tasks. Privileged credential abuse is a major factor in <a href=""https://www.infosecinstitute.com/resources/secure-coding/least-privilege-vulnerabilities-exploitation-case-study/#:%7E:text=rights%20to%20the%20minimum%20they,need%20to%20perform%20their%20job"">security breaches</a>, with privilege escalation being a common attack vector. Attackers often start with low-level access and exploit software vulnerabilities or misconfigurations to gain administrator privileges, especially when accounts have excessive or unnecessary rights. Another overlooked risk is standing privileges — continuously available privileged access that expands the attack surface. <strong>Just-in-time privileged access management (JIT PAM)</strong> mitigates this by granting access only when needed and revoking it immediately after, minimizing exposure. A true least-privilege security model ensures that users, applications and systems have only the necessary rights for the shortest required duration — a critical requirement for compliance and regulatory security. Our teams have implemented this through an automated workflow that triggers a lightweight approval process, assigns temporary roles with restricted access and enforces time to live (TTL) for each role, ensuring privileges expire automatically once the task is completed.</p>"
Model distillation,Contain,Techniques,TRUE,new,"<p><a href=""https://openai.com/index/scaling-laws-for-neural-language-models/"">Scaling laws</a> have been a key driver of the AI boom — the principle that larger models, datasets and compute resources lead to more powerful AI systems. However, consumer hardware and edge devices often lack the capacity to support large-scale models, creating the need for model distillation.</p>

<p><strong><a href=""https://en.wikipedia.org/wiki/Knowledge_distillation"">Model distillation</a></strong> transfers knowledge from a larger, more powerful model (teacher) to a smaller, cost-efficient model (student). The process typically involves generating a sample dataset from the teacher model and fine-tuning the student to capture its statistical properties. Unlike pruning or <a href=""https://www.ibm.com/think/topics/quantization"">quantization</a>, which focus on compressing models by removing parameters, distillation aims to retain domain-specific knowledge, minimizing accuracy loss. It can also be combined with quantization for further optimization.</p>

<p><a href=""https://arxiv.org/abs/1503.02531"">Originally proposed</a> by Geoffrey Hinton et al., model distillation has gained widespread adoption. A notable example is the Qwen/Llama distilled version of <a href=""/radar/platforms/deepseek-r1"">DeepSeek R1</a>, which preserves strong reasoning capabilities in smaller models. With its growing maturity, the technique is no longer confined to research labs; it’s now being applied to everything from industrial to personal projects. Providers such as <a href=""https://platform.openai.com/docs/guides/distillation"">OpenAI</a> and <a href=""https://aws.amazon.com/bedrock/model-distillation/"">Amazon Bedrock</a> offer guides to help developers distill their own <a href=""/radar/techniques/small-language-models"">small language models</a> (SLMs). We believe adopting model distillation can help organizations manage LLM deployment costs while unlocking the potential of <a href=""/radar/techniques/on-device-llm-inference"">on-device LLM inference</a>.</p>"
Prompt engineering,Contain,Techniques,FALSE,moved in,"<p><strong>Prompt engineering</strong> refers to the process of designing and refining prompts for generative AI models to produce high-quality, context-aware responses. This involves crafting clear, specific and relevant prompts tailored to the task or application to optimize the model’s output. As LLM capabilities evolve, particularly with the emergence of <a href=""/radar/platforms/reasoning-models"">reasoning models</a>, prompt engineering practices must also adapt. Based on our experience with AI code generation, we’ve observed that <a href=""https://www.promptingguide.ai/techniques/fewshot"">few-shot prompting</a> may underperform compared to simple zero-shot prompting when working with reasoning models. Additionally, the widely used <a href=""https://arxiv.org/abs/2201.11903"">chain-of-thought</a> (CoT) prompting can <a href=""https://arxiv.org/abs/2410.21333"">degrade</a> reasoning model performance — likely because reinforcement learning has already <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"">fine-tuned</a> their built-in CoT mechanism.</p>

<p>Our hands-on experience aligns with academic research, which suggests ""advanced models may <a href=""https://arxiv.org/abs/2411.02093"">eliminate the need</a> for prompt engineering in software engineering."" However, traditional prompt engineering techniques still play a crucial role in reducing hallucinations and improving output quality, especially given the differences in response time and token costs between reasoning models and general LLMs. When building <a href=""/radar/techniques/llm-powered-autonomous-agents"">agentic applications</a>, we recommend choosing models strategically, based on your needs, while continuing to refine your prompt templates and corresponding techniques. Striking the right balance among performance, response time and token cost remains key to maximizing LLM effectiveness.</p>"
Small language models,Contain,Techniques,FALSE,no change,"<p>The recent announcement of <a href=""/radar/platforms/deepseek-r1"">DeepSeek R1</a> is a great example of why <strong>small language models</strong> (SLMs) continue to be interesting. The full-size R1 has 671 billion parameters and requires about 1,342 GB of VRAM in order to run, only achievable using a ""mini cluster"" of eight state-of-the-art NVIDIA GPUs. But <a href=""https://www.thoughtworks.com/en-ca/insights/blog/generative-ai/demystifying-deepseek"">DeepSeek</a> is also available <a href=""https://venturebeat.com/ai/deepseeks-r1-and-openais-deep-research-just-redefined-ai-rag-distillation-and-custom-models-will-never-be-the-same/"">""distilled""</a> into Qwen and Llama — smaller, open-weight models — effectively transferring its abilities and allowing it to be run on much more modest hardware. Though the model gives up some performance at those smaller sizes, it still allows a huge leap in performance over previous SLMs. The SLM space continues to innovate elsewhere, too. Since the last Radar, Meta introduced <a href=""https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"">Llama 3.2</a> at 1B and 3B sizes, Microsoft released <a href=""https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090"">Phi-4</a>, offering high-quality results with a 14B model, and Google released <a href=""https://developers.googleblog.com/en/introducing-paligemma-2-mix/"">PaliGemma 2</a>, a vision-language model at 3B, 10B and 28B sizes. These are just a few of the models currently being released at smaller sizes and definitely an important trend to continue to watch.</p>"
Using GenAI to understand legacy codebases,Contain,Techniques,FALSE,no change,"<p>In the past few months, <strong>using GenAI to understand legacy codebases</strong> has made some real progress. Mainstream tools such as GitHub Copilot are being touted as <a href=""https://github.blog/ai-and-ml/github-copilot/modernizing-legacy-code-with-github-copilot-tips-and-examples/"">being able to help modernize legacy codebases</a>. Tools such as Sourcegraph's <a href=""https://sourcegraph.com/cody"">Cody</a> are making it easier for developers to navigate and understand entire codebases. These tools use a multitude of GenAI techniques to provide contextual help, simplifying work with complex legacy systems. On top of that, specialized frameworks like <a href=""https://arxiv.org/abs/2403.10588"">S3LLM</a> are showing how LLMs can handle large-scale scientific software — such as that written in Fortran or Pascal — bringing GenAI-enhanced understanding to codebases outside of traditional enterprise IT. We think this technique is going to continue to gain traction given the sheer amount of legacy software in the world.</p>"
AI-friendly code design,Assess,Techniques,TRUE,new,"<p>Supervised <a href=""/radar/techniques/software-engineering-agents"">software engineering agents</a> are increasingly capable of identifying necessary updates and making larger changes to a codebase. At the same time, we're seeing growing <a href=""/radar/techniques/complacency-with-ai-generated-code"">complacency with AI-generated code</a>, and developers becoming reluctant to review large AI-made change sets. A common justification for this is that human-oriented code quality matters less since AI can handle future modifications; however, AI coding assistants also perform better with well-factored codebases, making <strong>AI-friendly code design</strong> crucial for maintainability.</p>

<p>Fortunately, good software design for humans also benefits AI. Expressive naming provides domain context and functionality; modularity and abstractions keep AI’s context manageable by limiting necessary changes; and the DRY (don’t repeat yourself) principle reduces duplicate code — making it easier for AI to keep the behavior consistent. So far, the best AI-friendly patterns align with established best practices. As AI evolves, expect more AI-specific patterns to emerge, so thinking about code design with this in mind will be extremely helpful.</p>"
AI-powered UI testing,Assess,Techniques,TRUE,new,"<p>New techniques for AI-powered assistance on software teams are emerging beyond just code generation. One area gaining traction is <strong>AI-powered UI testing</strong>, leveraging LLMs' abilities to interpret graphical user interfaces. There are several approaches to this. One category of tools uses multi-modal LLMs fine-tuned for UI snapshot processing, allowing test scripts written in natural language to navigate an application. Examples in this space include <a href=""https://qa.tech/"">QA.tech</a> or <a href=""https://www.lambdatest.com/kane-ai"">LambdaTests' KaneAI</a>. Another approach, seen in <a href=""/radar/Solutions/browser-use"">Browser Use</a>, combines multi-modal foundation models with <a href=""/radar/Solutions/playwright"">Playwright</a>'s insights into a web page's structure rather than relying on fine-tuned models.</p>

<p>When integrating AI-powered UI tests into a test strategy, it’s crucial to consider where they provide the most value. These methods can complement manual exploratory testing, and while the non-determinism of LLMs may introduce flakiness, their fuzziness can be an advantage. This could be useful for testing legacy applications with missing selectors or applications that frequently change labels and click paths.</p>"
Competence envelope as a model for understanding system failures,Assess,Techniques,TRUE,new,"<p><a href=""https://www.researchgate.net/publication/327427067_The_Theory_of_Graceful_Extensibility_Basic_rules_that_govern_adaptive_systems"">The theory of graceful extensibility</a> defines the basic rules governing adaptive systems, including the socio-technical systems involved in building and operating software. A key concept in this theory is the <strong>competence envelope</strong> — the boundary within which a system can function <em>robustly</em> in the face of failure. When a system is pushed beyond its competence envelope, it becomes brittle and is more likely to fail. This model provides a valuable lens for understanding system failure, as seen in the <a href=""https://surfingcomplexity.blog/2024/12/21/the-canva-outage-another-tale-of-saturation-and-resilience/"">complex failures</a> that led to the 2024 Canva outage. <a href=""https://www.sciencedirect.com/science/article/pii/S1877050920305585"">Residuality theory</a>, a recent development in software architecture thinking, offers a way to test a system’s competence envelope by deliberately introducing stressors and analyzing how the system has adapted to historical stressors over time. The approaches align with concepts of anti-fragility, resilience and robustness in socio-technical systems, and we’re eager to see practical applications of these ideas emerge in the field.</p>"
Structured output from LLMs,Assess,Techniques,FALSE,stay,"<p><strong>Structured output from LLMs</strong> refers to the practice of constraining a language model's response into a defined schema. This can be achieved either by instructing a generalized model to respond in a particular format or by fine-tuning a model so it ""natively"" outputs, for example, JSON. OpenAI now supports structured output, allowing developers to supply a JSON Schema, <a href=""/radar/Solutions/pydantic"">pydantic</a> or Zod object to constrain model responses. This capability is particularly valuable for enabling function calling, API interactions and external integrations, where accuracy and adherence to a format are critical. Structured output not only enhances the way LLMs can interface with code but also supports broader use cases like generating markup for rendering charts. Additionally, structured output has been shown to reduce the chance of hallucinations in model outputs.</p>"
AI-accelerated shadow IT,End-of-Life,Techniques,TRUE,new,"<p>AI is lowering the barriers for noncoders to build and integrate software themselves, instead of waiting for the IT department to get around to their requirements. While we’re excited about the potential this unlocks, we’re also wary of the first signs of <strong>AI-accelerated shadow IT</strong>. No-code workflow automation platforms now support AI API integration (e.g., OpenAI or Anthropic), making it tempting to use AI as duct tape — stitching together integrations that previously weren’t possible, such as turning chat messages in one system into ERP API calls via AI. At the same time, AI coding assistants are becoming more agentic, enabling noncoders with basic training to build internal utility applications.</p>

<p>This has all the hallmarks of the next evolution of the spreadsheets that still power critical processes in some enterprises — but with a much bigger footprint. Left unchecked, this new shadow IT could lead to a proliferation of ungoverned, potentially insecure applications, scattering data across more and more systems. Organizations should be aware of these risks and carefully weigh the trade-offs between rapid problem-solving and long-term stability.</p>"
Complacency with AI-generated code,End-of-Life,Techniques,FALSE,no change,"<p>As AI coding assistants continue to gain traction, so does the growing body of data and research highlighting concerns about <strong>complacency with AI-generated code</strong>. GitClear's latest <a href=""https://www.gitclear.com/ai_assistant_code_quality_2025_research"">code quality research</a> shows that in 2024, duplicate code and code churn have increased even more than predicted, while refactoring activity in commit histories has declined. Also reflecting AI complacency, <a href=""https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/"">Microsoft research</a> on knowledge workers found that AI-driven confidence often comes at the expense of critical thinking — a pattern we’ve observed as complacency sets in with prolonged use of coding assistants. The rise of supervised <a href=""/radar/techniques/software-engineering-agents"">software engineering agents</a> further amplifies the risks, because when AI generates larger and larger change sets, developers face greater challenges in reviewing results. The emergence of ""<a href=""https://arstechnica.com/ai/2025/03/is-vibe-coding-with-ai-gnarly-or-reckless-maybe-some-of-both/"">vibe coding</a>"" — where developers let AI generate code with minimal review — illustrates the growing trust of AI-generated outputs. While this approach can be appropriate for things like prototypes or other types of throw-away code, we strongly caution against using it for production code.</p>"
Local coding assistants,End-of-Life,Techniques,TRUE,new,"<p>Organizations remain wary of third-party AI coding assistants, particularly due to concerns about code confidentiality. As a result, many developers are considering using <strong>local coding assistants</strong> — AI that runs entirely on their machines — eliminating the need to send code to external servers. However, local assistants still lag behind their cloud-based counterparts, which rely on larger, more capable models. Even on high-end developer machines, smaller models remain limited in their capabilities. We've found that they struggle with complex prompts, lack the necessary context window for larger problems and often cannot trigger tool integrations or function calls. These capabilities are especially essential to <a href=""/radar/techniques/software-engineering-agents"">agentic workflows</a>, which is the cutting edge in coding assistance right now.</p>

<p>So while we recommend to proceed with low expectations, there are some capabilities that are valid locally. Some popular IDEs do now embed smaller models into their core features, such as Xcode's predictive code completion and <a href=""https://blog.jetbrains.com/blog/2024/04/04/full-line-code-completion-in-jetbrains-ides-all-you-need-to-know"">JetBrains' full-line code completion</a>. And locally runnable LLMs like <a href=""https://ollama.com/library/qwen2.5-coder"">Qwen Coder</a> are a step forward for local inline suggestions and handling simple coding queries. You can test these capabilities with <a href=""/radar/tools/continue"">Continue</a>, which supports the integration of local models via runtimes like <a href=""/radar/tools/ollama"">Ollama</a>.</p>"
Replacing pair programming with AI,End-of-Life,Techniques,FALSE,stay,"<p>When people talk about coding assistants, the topic of <a href=""https://martinfowler.com/articles/on-pair-programming.html"">pair programming</a> inevitably comes up. Our profession has a love-hate relationship with it: some swear by it, others can't stand it. Coding assistants now raise the question: can a human pair with the AI instead of another human and get the same results for the team? <a href=""/radar/tools/github-copilot"">GitHub Copilot</a> even calls itself ""your AI pair programmer."" While we do think a coding assistant can bring some of the benefits of pair programming, we advise against fully <strong><a href=""https://martinfowler.com/articles/exploring-gen-ai.html#memo-05"">replacing pair programming with AI</a></strong>. Framing coding assistants as pair programmers ignores one of the key benefits of pairing: to make the team, not just the individual contributors, better. Coding assistants can offer benefits for getting unstuck, learning about a new technology, onboarding or making tactical work faster so that we can focus on the strategic design. But they don't help with any of the team collaboration benefits, like keeping the work-in-progress low, reducing handoffs and relearning, making continuous integration possible or improving collective code ownership.</p>"
Reverse ETL,End-of-Life,Techniques,TRUE,new,"<p>We're seeing a worrying proliferation of so-called <strong>Reverse ETL</strong>. Regular ETL jobs have their place in traditional data architectures, where they transfer data from transaction processing systems to a centralized analytics system, such as a data warehouse or data lake. While this architecture has well-documented shortcomings, many of which are addressed by a <a href=""/radar/techniques/data-mesh"">data mesh</a>, it remains common in enterprises. In such an architecture, moving data back from a central analytics system to a transaction system makes sense in certain cases — for example, when the central system can aggregate data from multiple sources or as part of a <a href=""/radar/techniques/transitional-architecture"">transitional architecture</a> when migrating toward a data mesh. However, we're seeing a growing trend where product vendors use Reverse ETL as an excuse to move increasing amounts of business logic into a centralized platform — their product. This approach exacerbates many of the issues caused by centralized data architectures, and we suggest exercising extreme caution when introducing data flows from a sprawling, central data platform to transaction processing systems.</p>"
SAFe™,End-of-Life,Techniques,FALSE,no change,"<p>We see continued adoption of <strong><a href=""http://www.scaledagileframework.com/"">SAFe™</a></strong> (Scaled Agile Framework®). We also continue to observe that SAFe's over-standardized, phase-gated processes create friction, that it can promote silos and that its top-down control generates waste in the value stream and discourages engineering talent creativity, while limiting autonomy and experimentation in teams. A key reason for adoption is the complexity of making an organization agile, with enterprises hoping that a framework like SAFe offers a simple, process-based shortcut to becoming agile. Given the widespread adoption of SAFe — including among our clients — we’ve trained over 100 Thoughtworks consultants to better support them. Despite this in-depth knowledge and no lack of trying we come away thinking that sometimes there just is no simple solution to a complex problem, and we keep recommending <a href=""https://www.thoughtworks.com/insights/blog/digital-transformation/how-value-slices-can-fix-your-digital-transformation"">leaner, value-driven approaches and governance</a> that work in conjunction with a comprehensive change program.</p>

<p>Scaled Agile Framework® and SAFe™ are trademarks of Scaled Agile, Inc.</p>"
GitLab CI/CD,Adopt,Platforms,FALSE,moved in,"<p><strong><a href=""https://docs.gitlab.com/ee/ci/"">GitLab CI/CD</a></strong> has evolved into a fully integrated system within GitLab, covering everything from code integration and testing to deployment and monitoring. It supports complex workflows with features like multi-stage pipelines, caching, parallel execution and auto-scaling runners and is suitable for large-scale projects and complex pipeline needs. We want to highlight its built-in security and compliance tools (such as <a href=""https://about.gitlab.com/topics/devsecops/sast-vs-dast/"">SAST and DAST</a> analysis) which make it well-suited for use cases with high-compliance requirements. It also integrates seamlessly with Kubernetes, supporting cloud-native workflows, and offers real-time logging, test reports and traceability for enhanced observability.</p>"
Trino,Adopt,Platforms,FALSE,moved in,"<p><strong><a href=""https://trino.io/"">Trino</a></strong> is an open-source, distributed SQL query engine designed for interactive analytic queries over big data. It’s optimized to run both on-premise and cloud environments and supports querying data where it resides, including relational databases and various proprietary datastores via connectors. Trino can also query data stored in file formats like <a href=""https://parquet.apache.org/"">Parquet</a> and open-table formats like <a href=""/radar/platforms/apache-iceberg"">Apache Iceberg</a>. Its built-in query federation capabilities enable data from multiple sources to be queried as a single logical table, making it a great choice for analytic workloads that require aggregating data across diverse sources. Trino is a key part of popular stacks like <a href=""https://aws.amazon.com/athena/"">AWS Athena</a>, Starburst and other proprietary data platforms. Our teams have successfully used it in various use cases, and when it comes to querying data sets across multiple sources for analytics, Trino has been a reliable choice.</p>"
ABsmartly,Contain,Platforms,TRUE,new,"<p><strong><a href=""https://absmartly.com/"">ABsmartly</a></strong> is an advanced A/B testing and experimentation platform designed for rapid, trustworthy decision-making. Its standout feature is the <a href=""https://absmartly.com/gst"">Group Sequential Testing</a> (GST) engine, which accelerates test results by up to 80% compared to traditional A/B testing tools. The platform offers real-time reporting, deep data segmentation and seamless full-stack integration through an API-first approach, supporting experiments across web, mobile, microservices and ML models.</p>

<p>ABsmartly addresses key challenges in scalable, data-driven experimentation by enabling faster iteration and more agile product development. Its zero-lag execution, deep segmentation capabilities and support for multi-platform experiments make it particularly valuable for organizations looking to scale their experimentation culture and prioritize data-backed innovation. By significantly reducing test cycles and automating result analysis, ABsmartly helped us optimize features and user experiences more efficiently than traditional A/B testing platforms.</p>"
Dapr,Contain,Platforms,FALSE,moved in,"<p><strong><a href=""https://dapr.io/"">Dapr</a></strong> has evolved considerably since we last featured it in the Radar. Its many new features include job scheduling, virtual actors as well as more sophisticated retry policies and observability components. Its list of building blocks continues to grow with jobs, cryptography and more. Our teams also note its increasing focus on secure defaults, with support for mTLS and distroless images. All in all, we've been happy with Dapr and are looking forward to future developments.</p>"
Grafana Alloy,Contain,Platforms,TRUE,new,"<p>Formerly known as Grafana Agent, <strong><a href=""https://grafana.com/docs/alloy/latest/"">Grafana Alloy</a></strong> is an open-source <a href=""/radar/Solutions/opentelemetry"">OpenTelemetry</a> Collector. Alloy is designed to be an all-in-one telemetry collector for all telemetry data, including logs, metrics and traces. It supports collecting commonly used telemetry data formats such as OpenTelemetry, <a href=""/radar/tools/prometheus"">Prometheus</a> and Datadog. With <a href=""https://grafana.com/docs/loki/latest/send-data/promtail/"">Promtail’s recent deprecation</a>, Alloy is emerging as a go-to choice for telemetry data collection — especially for logs — if you’re using the Grafana observability stack.</p>"
Grafana Loki,Contain,Platforms,TRUE,new,"<p><strong><a href=""https://grafana.com/docs/loki/"">Grafana Loki</a></strong> is a horizontally scalable and highly available multi-tenant log aggregation system inspired by Prometheus. Loki only indexes metadata about your logs as a set of labels for each log stream. Log data is stored in a block storage solution such as S3, GCS or Azure Blob Storage. The upshot is that Loki promises a reduction in operational complexity and storage costs over competitors. As you'd expect, it integrates tightly with Grafana and <a href=""/radar/Solutions/grafana-alloy"">Grafana Alloy</a>, although other collection mechanisms can be used.</p>

<p>Loki 3.0 introduced native <a href=""/radar/Solutions/opentelemetry"">OpenTelemetry</a> support, making ingestion and integration with OpenTelemetry systems as simple as configuring an endpoint. It also offers advanced multi-tenancy features, such as tenant isolation via shuffle-sharding, which prevents misbehaving tenants (e.g., heavy queries or outages) from impacting others in a cluster. If you haven't been following developments in the Grafana ecosystem, now is a great time to take a look as it is evolving rapidly.</p>"
Grafana Tempo,Contain,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/grafana/tempo"">Grafana Tempo</a></strong> is a high-scale distributed tracing backend that supports open standards like <a href=""/radar/Solutions/opentelemetry"">OpenTelemetry</a>. Designed to be cost-efficient, it relies on object storage for long-term trace retention and enables trace search, <a href=""https://grafana.com/docs/tempo/latest/metrics-generator/span_metrics/"">span-based metric generation</a> and correlation with logs and metrics. By default, Tempo uses a <a href=""https://grafana.com/docs/tempo/latest/operations/schema/"">columnar block format</a> based on <a href=""https://github.com/grafana/tempo/blob/main/docs/design-proposals/2022-04%20Parquet.md"">Apache Parquet</a>, enhancing query performance and enabling downstream tools to access trace data. Queries are executed via <a href=""https://grafana.com/docs/tempo/latest/traceql/"">TraceQL</a> and the <a href=""https://grafana.com/docs/tempo/latest/operations/tempo_cli/"">Tempo CLI</a>. <a href=""/radar/platforms/grafana-alloy"">Grafana Alloy</a> too can be configured to collect and forward traces to Tempo. Our teams self-hosted Tempo in <a href=""/radar/platforms/gke"">GKE</a>, using <a href=""/radar/platforms/minio"">MinIO</a> for object storage, <a href=""/radar/Solutions/opentelemetry"">OpenTelemetry</a> collectors and <a href=""https://github.com/grafana/grafana"">Grafana</a> for trace visualization.</p>"
Railway,Contain,Platforms,TRUE,new,"<p><a href=""/radar/platforms/heroku"">Heroku</a> used to be an excellent choice for many developers who wanted to release and deploy their applications quickly. In recent years, we’ve also seen the rise of deployment platforms like <a href=""/radar/platforms/vercel"">Vercel</a>, which are more modern, lightweight and easy to use but designed for front-end applications. A full-stack alternative in this space is <strong><a href=""https://railway.com/"">Railway</a></strong>, a PaaS cloud platform that streamlines everything from GitHub/Docker deployment to production observability.</p>

<p>Railway supports most mainstream programming frameworks, databases as well as containerized deployment. As a long-term hosted platform for an application, you may need to compare the costs of different platforms carefully. At present, our team has had a good experience with Railway's deployment and observability. The operation is smooth and can be well integrated with the <a href=""/radar/techniques/continuous-deployment"">continuous deployment</a> practices we advocate.</p>"
Unblocked,Contain,Platforms,FALSE,moved in,"<p><strong><a href=""https://getunblocked.com"">Unblocked</a></strong> is an off-the-shelf <a href=""/radar/techniques/summary/ai-team-assistants"">AI team assistant</a>. Once integrated with codebase repositories, corporate documentation platforms, project management tools and communication tools, Unblocked helps answer questions about complex business and technical concepts, architectural design and implementation as well as operational processes. This is particularly useful for navigating large or legacy systems. While using Unblocked, we've observed that teams value quick access to contextual information over code and user-story generation. For scenarios requiring more extensive code generation or task automation, dedicated <a href=""/radar/tools/software-engineering-agents"">software engineering agents</a> or coding assistants are more suitable.</p>"
Weights & Biases,Contain,Platforms,FALSE,no change,"<p><strong><a href=""https://wandb.ai/"">Weights & Biases</a></strong> has continued to evolve, adding more LLM-focused features since it was last featured in the Radar. They are expanding <a href=""https://wandb.ai/site/traces"">Traces</a> and introducing <a href=""https://wandb.ai/site/weave/"">Weave</a>, a full-fledged platform that goes beyond tracking LLM-based agentic systems. Weave enables you to create system evaluations, define custom metrics, use LLMs as judges for tasks like summarization and save data sets that capture different behaviors for analysis. This helps optimize LLM components and track performance at both local and global levels. The platform also facilitates iterative development and effective debugging of agentic systems, where errors can be difficult to detect. Additionally, it enables the collection of valuable human feedback, which can later be used for fine-tuning models.</p>"
Arize Phoenix,Assess,Platforms,TRUE,new,"<p>With the popularity of LLM and <a href=""/radar/techniques/llm-powered-autonomous-agents"">agentic applications</a>, LLM observability is becoming more and more important. Previously, we’ve recommended platforms such as <a href=""/radar/platforms/langfuse"">Langfuse</a> and <a href=""/radar/platforms/weights-biases"">Weights & Biases (W&B)</a>. <strong><a href=""https://github.com/Arize-ai/phoenix"">Arize Phoenix</a></strong> is ​​another emerging platform in this space, and our team has had a positive experience using it. It offers standard features like LLM tracing, evaluation and prompt management, with seamless <a href=""https://docs.arize.com/phoenix/tracing/integrations-tracing"">integration</a> into leading LLM providers and frameworks. This makes it easy to gather insights on LLM output, latency and token usage with minimal configuration. So far, our experience is limited to the open-source tool but the broader <a href=""https://arize.com/"">Arize</a> platform offers more comprehensive capabilities. We look forward to exploring it in the future.</p>"
Chainloop,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/chainloop-dev/chainloop"">Chainloop</a></strong> is an open-source supply chain security platform that helps security teams enforce compliance while allowing development teams to seamlessly integrate security compliance into CI/CD pipelines. It consists of a control plane, which acts as the single source of truth for security policies, and a CLI, which runs attestations within <a href=""https://github.com/chainloop-dev/chainloop/blob/main/docs/examples/ci-workflows/github.yaml"">CI/CD workflows</a> to ensure compliance. Security teams define <a href=""https://docs.chainloop.dev/getting-started/workflow-definition#workflow-contracts"">workflow contracts</a> specifying which artifacts — such as <a href=""/radar/techniques/software-bill-of-materials"">SBOMs</a> and vulnerability reports — must be collected, where to store them and how to evaluate compliance. Chainloop uses <a href=""https://www.openpolicyagent.org/docs/latest/policy-language/"">Rego</a>, <a href=""/radar/tools/open-policy-agent-opa"">OPA's</a> policy language, to validate attestations — for example, ensuring a <a href=""/radar/platforms/cyclonedx"">CycloneDX</a> SBOM meets version requirements. During workflow execution, security artifacts like <a href=""https://docs.chainloop.dev/guides/sbom-management/"">SBOMs</a> are attached to an attestation and pushed to the control plane for enforcement and auditing. This approach ensures compliance can be enforced consistently and at scale while minimizing friction in development workflows. This results in an <a href=""/radar/techniques/slsa"">SLSA</a> level-three–compliant single source of truth for metadata, artefacts and attestations.</p>"
Deepseek R1,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/deepseek-ai/DeepSeek-R1"">DeepSeek-R1</a></strong> is DeepSeek's first-generation of <a href=""/radar/platforms/reasoning-models"">reasoning models</a>. Through a progression of non-reasoning models, the engineers at DeepSeek designed and used methods to maximize hardware utilization. These include Multi-Head Latent Attention (MLA), Mixture of Experts (MoE) gating, 8-bit floating points training (FP8) and low-level PTX programming. Their <a href=""https://martinfowler.com/articles/deepseek-papers.html#Deepseek-v3HpcCo-design"">high-performance computing co-design</a> approach enables DeepSeek-R1 to rival state-of-the-art models at significantly reduced cost for training and inference.</p>

<p>DeepSeek-R1-Zero is notable for another innovation: the engineers were able to elicit reasoning capabilities from a non-reasoning model using simple reinforcement learning without any supervised fine-tuning. All DeepSeek models are open-weight, which means they are freely available, though training code and data remain proprietary. The repository includes six dense models distilled from DeepSeek-R1, based on Llama and Qwen, with DeepSeek-R1-Distill-Qwen-32B outperforming OpenAI-o1-mini on various benchmarks.</p>"
Deno,Assess,Platforms,FALSE,no change,"<p>Created by Ryan Dahl, the inventor of Node.js, <strong><a href=""https://deno.com/"">Deno</a></strong> was designed to address what he saw as mistakes in Node.js. It features a stricter sandboxing system, built-in dependency management and native TypeScript support — a key draw for its user base. Many of us prefer Deno for TypeScript projects, as it feels like a true TypeScript run time and toolchain, rather than an add-on to Node.js.</p>

<p>Since its inclusion in <a href=""/radar/platforms/deno"">the Radar in 2019</a>, Deno has made significant advancements. The <a href=""https://deno.com/blog/v2.0"">Deno 2 release</a> introduces backward compatibility with <a href=""/radar/platforms/node-js"">Node.js</a> and npm libraries, long-term support (LTS) releases and other improvements. Previously, one of the biggest barriers to adoption was the need to rewrite Node.js applications. These updates reduce migration friction while expanding dependency options for supporting tools and systems. Given the massive Node.js and npm ecosystem, these changes should drive further adoption.</p>

<p>Additionally, Deno’s <a href=""https://deno.com/blog/v2.0#the-standard-library-is-now-stable"">Standard Library</a> has stabilized, helping combat the proliferation of <a href=""https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/"">low-value npm packages across the ecosystem</a>. Its tooling and Standard Library make TypeScript or JavaScript more appealing for server-side development. However, we caution against choosing a platform solely to avoid <a href=""/radar/techniques/polyglot-programming"">polyglot programming</a>.</p>"
Graphiti,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/getzep/graphiti"">Graphiti</a></strong> builds dynamic, temporally-aware knowledge graphs that capture evolving facts and relationships. Our teams use <a href=""/radar/techniques/graphrag"">GraphRAG</a> to uncover data relationships, which enhances retrieval and response accuracy. As data sets constantly evolve, Graphiti maintains temporal metadata on graph edges to record relationship lifecycles. It ingests both structured and unstructured data as discrete <a href=""https://help.getzep.com/graphiti/graphiti/adding-episodes"">episodes</a> and supports queries using a fusion of time-based, full-text, semantic and graph algorithms. For LLM-based applications — whether <a href=""/radar/techniques/retrieval-augmented-generation-rag"">RAG</a> or <a href=""/radar/techniques/llm-powered-autonomous-agents"">agentic</a> — Graphiti enables long-term recall and state-based reasoning.</p>"
Helicone,Assess,Platforms,TRUE,new,"<p>Similar to <a href=""/radar/platforms/langfuse"">Langfuse</a>, <a href=""/radar/platforms/weights-biases"">Weights & Biases</a> and <a href=""/radar/platforms/arize-phoenix"">Arize Phoenix</a>, <strong><a href=""https://www.helicone.ai/"">Helicone</a></strong> is a managed LLMOps platform designed to meet the growing enterprise demand for LLM cost management, ROI evaluation and risk mitigation. Open-source and developer-focused, Helicone supports production-ready AI applications, offering prompt experimentation, monitoring, debugging and optimization across the entire LLM lifecycle. It enables real-time analysis of costs, utilization, performance and agentic stack traces across various LLM providers. While it simplifies LLM operations management, the platform is still emerging and may require some expertise to fully leverage its advanced features. Our team has been using it with good experience so far.</p>"
Humanloop,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://humanloop.com/"">Humanloop</a></strong> is an emerging platform focused on making AI systems more reliable, adaptable and aligned with user needs by integrating human feedback at key decision points. It offers tools for human labeling, active learning and human-in-the-loop fine-tuning as well as LLM evaluation against business requirements. Additionally, it helps manage the cost-effective lifecycle of GenAI solutions with greater control and efficiency. Humanloop supports collaboration through a shared workspace, version-controlled prompt management and CI/CD integration to prevent regressions. It also provides observability features such as tracing, logging, alerting and guardrails to monitor and optimize AI performance. These capabilities make it particularly relevant for organizations deploying AI in regulated or high-risk domains where human oversight is critical. With its focus on responsible AI practices, Humanloop is worth evaluating for teams looking to build scalable and ethical AI systems.</p>"
Model Context Protocol (MCP),Assess,Platforms,TRUE,new,"<p>One of the biggest challenges in prompting is ensuring the AI tool has access to all the context relevant to the task. Often, this context already exists within the systems we use all day: wikis, issue trackers, databases or observability systems. Seamless integration between AI tools and these information sources can significantly improve the quality of AI-generated outputs.</p>

<p>The <strong><a href=""https://github.com/modelcontextprotocol"">Model Context Protocol</a></strong> (MCP), an open standard released by Anthropic, provides a standardized framework for integrating LLM applications with external data sources and tools. It defines MCP servers and clients, where servers access the data sources and clients integrate and use this data to enhance prompts. Many coding assistants have already implemented MCP integration, allowing them to act as MCP clients. MCP servers can be run in two ways: Locally, as a Python or Node process running on the user’s machine, or remotely, as a server that the MCP client connects to via SSE (though we haven't seen any usage of the remote server variant yet). Currently, MCP is primarily used in the first way, with developers cloning open-source <a href=""https://mcpservers.org/"">MCP</a> <a href=""https://mcp.so/"">server</a> <a href=""https://smithery.ai/"">implementations</a>. While locally run servers offer a neat way to avoid third-party dependencies, they remain less accessible to nontechnical users and introduce challenges such as governance and update management. That said, it's easy to imagine how this standard could evolve into a more mature and user-friendly ecosystem in the future.</p>"
Open WebUI,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/open-webui/open-webui"">Open WebUI</a></strong> is an open-source, self-hosted AI platform with a versatile feature set. It supports OpenAI-compatible APIs and integrates with providers like <a href=""/radar/tools/openrouter"">OpenRouter</a> and <a href=""https://groq.com/groqcloud/"">GroqCloud</a>, among others. It can run entirely offline by connecting to local or <a href=""/radar/techniques/self-hosted-llms"">self-hosted</a> models via <a href=""/radar/tools/ollama"">Ollama</a>. Open WebUI includes a built-in capability for <a href=""/radar/techniques/retrieval-augmented-generation-rag"">RAG</a>, allowing users to interact with local and web-based documents in a chat-driven experience. It offers granular RBAC controls, enabling different models and platform capabilities for different user groups. The platform is extensible through <a href=""https://docs.openwebui.com/features/plugin/functions/"">Functions</a> — Python-based building blocks that customize and enhance its capabilities. Another key feature is <a href=""https://docs.openwebui.com/features/evaluation/"">model evaluation</a>, which includes a model arena for side-by-side comparisons of LLMs on specific tasks. Open WebUI can be deployed at various scales — as a personal AI assistant, a <a href=""/radar/techniques/ai-team-assistants"">team collaboration assistant</a> or an enterprise-grade AI platform.</p>"
pg_mooncake,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/Mooncake-Labs/pg_mooncake"">pg_mooncake</a></strong> is a PostgreSQL extension that adds columnar storage and vectorized execution. Columnstore tables are stored as Iceberg or Delta Lake tables in the local file system or S3-compatible cloud storage. pg_mooncake supports loading data from file formats like Parquet, CSV and even Hugging Face datasets. It can be a good fit for heavy data analytics that typically requires columnar storage, as it removes the need to add dedicated columnar store technologies into your stack.</p>"
Reasoning models,Assess,Platforms,TRUE,new,"<p>One of the most significant AI advances since the last Radar is the breakthrough and proliferation of <strong>reasoning models</strong>. Also marketed as ""thinking models,"" these models have achieved top human-level performance in <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"">benchmarks</a> like frontier mathematics and coding.</p>

<p>Reasoning models are usually trained through reinforcement learning or supervised fine-tuning, enhancing capabilities such as step-by-step thinking (<a href=""https://arxiv.org/abs/2201.11903"">CoT</a>), exploring alternatives (<a href=""https://arxiv.org/abs/2305.10601"">ToT</a>) and <a href=""https://arxiv.org/abs/2402.13035"">self-correction</a>. Examples include OpenAI’s <a href=""https://openai.com/o1/"">o1</a>/<a href=""https://openai.com/index/openai-o3-mini/"">o3</a>, <a href=""/radar/platforms/deepseek-r1"">DeepSeek R1</a> and <a href=""https://deepmind.google/technologies/gemini/flash-thinking/"">Gemini 2.0 Flash Thinking</a>. However, these models should be seen as a distinct category of LLMs rather than simply more advanced versions.</p>

<p>This increased capability comes at a cost. Reasoning models require longer response time and higher token consumption, leading us to jokingly call them ""Slower AI"" (as if current AI wasn’t slow enough). Not all tasks justify this trade-off. For simpler tasks like text summarization, content generation or fast-response chatbots, general-purpose LLMs remain the better choice. We advise using reasoning models in STEM fields, complex problem-solving and decision-making — for example, when using <a href=""/radar/techniques/llm-as-a-judge"">LLMs as judges</a> or improving explainability through explicit CoT outputs. At the time of writing, Claude 3.7 Sonnet, a hybrid reasoning model, had just been <a href=""https://www.anthropic.com/news/claude-3-7-sonnet"">released</a>, hinting at a possible fusion between traditional LLMs and reasoning models.</p>"
Restate,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://restate.dev/"">Restate</a></strong> is a durable execution platform, similar to <a href=""/radar/platforms/temporal"">Temporal</a>, developed by the original creators of Apache Flink. Feature-wise it offers workflows as code, stateful event processing, the saga pattern and durable state machines. Written in Rust and deployed as a single binary, it uses a distributed log to record events, implemented using a virtual consensus algorithm based on <strong><a href=""https://arxiv.org/pdf/1608.06696v1"">Flexible Paxos</a></strong>; this ensures durability in the event of node failure. SDKs are available for the usual suspects: Java, Go, Rust and TypeScript. We still maintain that it's best to avoid distributed transactions in distributed systems, because of both the additional complexity and the inevitable additional operational overhead involved. However, this platform is worth assessing if you can’t avoid distributed transactions in your environment.</p>"
Supabase,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://supabase.com/"">Supabase</a></strong> is an open-source <a href=""/radar/platforms/firebase"">Firebase</a> alternative for building scalable and secure backends. It offers a suite of integrated services, including a PostgreSQL database, authentication, instant APIs, Edge Functions, real-time subscriptions, storage and vector embeddings. Supabase aims to streamline back-end development, allowing developers to focus on building front-end experiences while leveraging the power and flexibility of open-source technologies. Unlike Firebase, Supabase is built on top of PostgreSQL. If you're working on prototyping or an MVP, Supabase is worth considering, as it will be easier to migrate to another SQL solution after the prototyping stage.</p>"
Synthesized,Assess,Platforms,TRUE,new,"<p>A common challenge in software development is generating test data for development and test environments. Ideally, test data should be as production-like as possible, while ensuring no personally identifiable or sensitive information is exposed. Though this may seem straightforward, test data generation is far from simple. That's why we’re interested in <strong><a href=""https://www.synthesized.io/"">Synthesized</a></strong> — a platform that can mask and subset existing production data or generate statistically relevant synthetic data. It integrates directly into build pipelines and offers privacy masking, providing per-attribute anonymization through irreversible data obfuscation techniques such as hashing, randomization and binning. Synthesized can also generate large volumes of synthetic data for performance testing. While it includes the obligatory GenAI features, its core functionality addresses a real and persistent challenge for development teams, making it worth exploring.</p>"
Tonic.ai,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://www.tonic.ai/"">Tonic.ai</a></strong> is part of a growing trend in platforms designed to generate realistic, de-identified synthetic data for development, testing and QA environments. Similar to <a href=""/radar/platforms/synthesized"">Synthesized</a>, Tonic.ai is a platform with a comprehensive suite of tools addressing various data synthesis needs in contrast to the library-focused approach of <a href=""/radar/Solutions/synthetic-data-vault"">Synthetic Data Vault</a>. Tonic.ai generates both structured and unstructured data, maintaining the statistical properties of production data while ensuring privacy and compliance through differential privacy techniques. Key features include automatic detection, classification and redaction of sensitive information in unstructured data, along with on-demand database provisioning via Tonic Ephemeral. It also offers Tonic Textual, a secure data lakehouse that helps AI developers leverage unstructured data for <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> systems and LLM fine-tuning. Teams looking to accelerate engineering velocity while generating scalable, realistic data — all while adhering to stringent data privacy requirements — should consider evaluating Tonic.ai.</p>"
turbopuffer,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://turbopuffer.com/"">turbopuffer</a></strong> is a serverless, multi-tenant search engine that seamlessly integrates vector and full-text search on object storage. We quite like its architecture and <a href=""https://turbopuffer.com/docs/tradeoffs"">design choices</a>, particularly its focus on durability, scalability and cost efficiency. By using object storage as a write-ahead log while keeping its query nodes stateless, it’s well-suited for high-scale search workloads.</p>

<p>Designed for performance and accuracy, turbopuffer delivers <a href=""https://turbopuffer.com/blog/native-filtering"">high recall</a> out of the box, even for complex filter-based queries. It caches cold query results on NVMe SSDs and keeps frequently accessed namespaces in memory, enabling low-latency search across billions of documents. This makes it ideal for <a href=""https://turbopuffer.com/docs/limits"">large-scale</a> document retrieval, vector search and <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> AI applications. However, its reliance on object storage introduces trade-offs in query latency, making it most effective for workloads that benefit from stateless, distributed compute. turbopuffer powers high-scale production systems like <a href=""/radar/tools/cursor"">Cursor</a> but is currently only available by <a href=""https://turbopuffer.com/join"">referral or invitation</a>.</p>"
VectorChord,Assess,Platforms,TRUE,new,"<p><strong><a href=""https://github.com/tensorchord/VectorChord"">VectorChord</a></strong> is a PostgreSQL extension for vector similarity search, developed by the creators of <a href=""https://github.com/tensorchord/pgvecto.rs"">pgvecto.rs</a> as its successor. It’s open source, compatible with <a href=""/radar/tools/pgvector"">pgvector</a> data types and designed for disk-efficient, high-performance vector search. It employs inverted file indexing (IVF) along with <a href=""https://arxiv.org/pdf/2405.12497"">RaBitQ</a> quantization to enable fast, scalable and accurate vector search while significantly reducing computation demands. Like other PostgresSQL extensions in this space, it leverages the PostgreSQL ecosystem, allowing vector search alongside standard transactional operations. Though still in its early stages, VectorChord is worth assessing for vector search workloads.</p>"
Tyk hybrid API management,End-of-Life,Platforms,TRUE,new,"<p>We've observed multiple teams encountering issues with the <strong><a href=""https://tyk.io/deployment-multi-cloud-and-hybrid/"">Tyk hybrid API management</a></strong> solution. While the concept of a managed control plane and self-managed data planes offers flexibility for complex infrastructure setups (such as multi-cloud and hybrid cloud), teams have experienced control plane incidents that were only discovered internally rather than by Tyk, highlighting potential observability gaps in Tyk's AWS-hosted environment. Furthermore, the level of incident support appears slow; communicating via tickets and emails isn’t ideal in these situations. Teams have also reported issues with the maturity of Tyk's documentation, often finding it inadequate for complex scenarios and issues. Additionally, other products in the Tyk ecosystem seem immature as well, for example, the enterprise developer portal is reported to not be backward compatible and has limited customization capabilities. Especially for Tyk’s hybrid setup, we recommend proceeding with caution and will continue to monitor its maturity.</p>"
Renovate,Adopt,Tools,FALSE,moved in,"<p><strong><a href=""https://www.mend.io/renovate/"">Renovate</a></strong> has become the tool of choice for many of our teams looking to take a proactive approach to dependency version management. While <a href=""/radar/tools/dependabot"">Dependabot</a> remains a safe default choice for GitHub-hosted repositories, we continue to recommend evaluating Renovate as a more comprehensive and customizable solution. To maximize Renovate’s benefits, configure it to monitor and update all dependencies, including tooling, infrastructure and private or internally hosted dependencies. To reduce developer fatigue, consider <a href=""/radar/techniques/automatic-merging-of-dependency-update-prs"">automatic merging of dependency update PRs</a>.</p>"
uv,Adopt,Tools,FALSE,moved in,"<p>Since the last Radar, we’ve gained more experience with <strong><a href=""https://github.com/astral-sh/uv"">uv</a></strong>, and feedback from teams has been overwhelmingly positive. uv is a next-generation Python package and project management tool written in <a href=""/radar/Solutions/rust"">Rust</a>, with a key value proposition: it’s ""extremely fast."" It outperforms other Python package managers by a large margin in benchmarks, accelerating build and test cycles and significantly improving developer experience. Beyond performance, uv offers a unified toolset, effectively replacing tools like Poetry, pyenv and pipx. However, our concerns around package management tools remain: a strong ecosystem, mature community and long-term support are critical. Given that uv is relatively new, moving it to the Adopt ring is bold. That said, many data teams are eager to <a href=""https://github.com/pytorch/pytorch/issues/138506"">move away</a> from Python’s legacy package management system, and our frontline developers consistently recommend uv as the best tool available today.</p>"
Vite,Adopt,Tools,FALSE,moved in,"<p>Since <strong><a href=""https://vite.dev/"">Vite</a></strong> was last mentioned in the Radar, it has gained even more traction. It’s a high-performance front-end build tool with fast hot-reloading. It’s being adopted and recommended as a default choice in many front-end frameworks, including Vue, SvelteKit and React which recently <a href=""https://react.dev/blog/2025/02/14/sunsetting-create-react-app"">deprecated create-react-app</a>. Vite also recently received significant investment, which led to <a href=""https://voidzero.dev/posts/announcing-voidzero-inc"">the founding of VoidZero</a>, an organization dedicated to Vite’s development. This investment should accelerate development and enhance the project's long-term sustainability.</p>"
Claude Sonnet,Contain,Tools,TRUE,new,"<p><strong><a href=""https://www.anthropic.com/news/claude-3-5-sonnet"">Claude Sonnet</a></strong> is an advanced language model that excels in coding, writing, analysis and visual processing. It's available in the browser, terminal, most major IDEs and even integrates with <a href=""https://www.anthropic.com/news/github-copilot"">GitHub Copilot</a>. As of writing, benchmarking shows it outperforms previous models with versions 3.5 and 3.7, including earlier Claude models. It's also adept at interpreting charts and extracting text from images, and it features a developer-focused experience, such as with the ""Artifacts"" feature in the browser UI for generating and interacting with dynamic content such as code snippets and HTML designs.</p>

<p>We’ve used version 3.5 of Claude Sonnet extensively in software development and found it significantly boosts productivity across various projects. It excels in greenfield projects, particularly in collaborative software design and architectural discussions. While it may be too early to call any AI model ""stable"" for coding assistance, Claude Sonnet is among the most reliable models we've worked with. At the time of writing, <a href=""https://www.anthropic.com/news/claude-3-7-sonnet"">Claude 3.7</a> has also been released and is promising, though we’ve not yet fully tested it in production.</p>"
Cline,Contain,Tools,TRUE,new,"<p><strong><a href=""https://github.com/cline/cline"">Cline</a></strong> is an open-source VSCode extension that is currently one of the strongest contenders in the space of supervised <a href=""/radar/tools/software-engineering-agents"">software engineering agents</a>. It lets developers drive their implementation entirely from the Cline chat, integrating seamlessly with the IDE they already use. Key features like Plan & Act mode, transparent token usage and <a href=""/radar/techniques/model-context-protocol-mcp"">MCP</a> integration help developers interact effectively with LLMs. Cline has demonstrated advanced capabilities in handling complex development tasks, especially with <a href=""/radar/tools/claude-3-5-sonnet"">Claude 3.5 Sonnet</a>. It supports large codebases, automates headless browser testing and proactively fixes bugs. Unlike cloud-based solutions, Cline enhances privacy by <a href=""https://github.com/cline/cline/blob/main/docs/PRIVACY.md"">storing data locally</a>. Its open-source nature not only ensures greater transparency but also enables community-driven improvements. However, developers should be mindful of token usage cost, as Cline's code context orchestration, while very effective, is resource-intensive. Another potential bottleneck is <a href=""https://github.com/cline/cline/issues/923"">rate limiting</a>, which can slow down workflows. Until this is resolved, using API providers like <a href=""/radar/tools/openrouter"">OpenRouter</a>, which provide better rate limits, is advisable.</p>"
Cursor,Contain,Tools,FALSE,moved in,"<p>We continue to be impressed by the AI-first code editor <strong><a href=""https://www.cursor.com/"">Cursor</a></strong>, which remains a leader in the competitive AI coding assistance space. Its code context orchestration is very effective, and it supports a wide range of models, including the option to use a custom API key. The Cursor team often comes up with innovative user experience features before the other vendors, and they include an extensive list of context providers in their chat, such as the referencing of git diffs, previous AI conversations, web search, library documentation and <a href=""/radar/platforms/model-context-protocol-mcp"">MCP</a> integration. Alongside tools like <a href=""/radar/tools/cline"">Cline</a> and <a href=""/radar/tools/windsurf"">Windsurf</a>, Cursor also stands out for its strong agentic coding mode. This mode allows developers to guide their implementation directly from an AI chat interface, with the tool autonomously reading and modifying files, as well as executing commands. Additionally, we appreciate Cursor's ability to detect linting and compilation errors in generated code and proactively correct them.</p>"
D2,Contain,Tools,TRUE,new,"<p><strong><a href=""https://github.com/terrastruct/d2"">D2</a></strong> is an open-source <a href=""/radar/techniques/diagrams-as-code"">diagrams-as-code</a> tool that helps users create and customize diagrams from text. It introduces the <a href=""https://d2lang.com/tour/intro"">D2 diagram scripting language</a>, which prioritizes readability over compactness with a simple, declarative syntax. D2 ships with a default <a href=""https://d2lang.com/tour/themes"">theme</a> and leverages the same <a href=""https://d2lang.com/tour/layouts"">layout engine</a> as <a href=""/radar/tools/mermaid"">Mermaid</a>. Our teams appreciate its lightweight syntax, which is specifically designed for software documentation and architecture diagrams.</p>"
Databricks Delta Live Tables,Contain,Tools,FALSE,moved in,"<p><strong><a href=""https://www.databricks.com/product/data-engineering/dlt"">Delta Live Tables</a></strong> (DLT) continues to prove its value in simplifying and streamlining data pipeline management, supporting both real-time streaming and batch processing through a declarative approach. By automating complex data engineering tasks, such as manual checkpoint management, DLT reduces operational overhead while ensuring a robust end-to-end system. Its ability to orchestrate simple pipelines with minimal manual intervention enhances reliability and flexibility, while features like materialized views provide incremental updates and performance optimization for specific use cases.</p>

<p>However, teams must understand DLT’s nuances to fully leverage its benefits and avoid potential pitfalls. As an opinionated abstraction, DLT manages its own tables and restricts data insertion to a single pipeline at a time. Streaming tables are append-only, requiring careful design considerations. Additionally, deleting a DLT pipeline also deletes the underlying table and data, potentially creating operational issues.</p>"
JSON Crack,Contain,Tools,TRUE,new,"<p><strong><a href=""https://marketplace.visualstudio.com/items?itemName=AykutSarac.jsoncrack-vscode"">JSON Crack</a></strong> is a Visual Studio Code extension that renders interactive graphs from textual data. Despite its name it supports multiple formats, including YAML, TOML and XML. Unlike <a href=""/radar/tools/mermaid"">Mermaid</a> and <a href=""/radar/tools/d2"">D2</a>, where the textual form is a means to create a specific visual graph, JSON Crack is a tool to look at data that happens to be in a textual format. The layout algorithm works well and the tool allows selective hiding of branches and nodes, making it a great choice for exploring data sets. A companion web-based tool is also available, but our reservations about relying on <a href=""/radar/tools/online-services-for-formatting-or-parsing-code"">online services for formatting or parsing code</a> apply. JSON Crack does have a node limit, and directs users to a commercial sibling tool for handling files with more than a few hundred nodes.</p>"
MailSlurp,Contain,Tools,TRUE,new,"<p>Testing workflows that involve email are often complex and time-consuming. Development teams must build custom email API clients for automation while also setting up temporary inboxes for manual testing scenarios, such as user testing or internal product training before major releases. These challenges become even more pronounced when developing customer onboarding products. We’ve had a positive experience with <strong><a href=""https://docs.mailslurp.com/"">MailSlurp</a></strong>, a mail server and SMS API service. It provides REST APIs for creating inboxes and phone numbers as well as validating emails and messages directly in code, and its no-code dashboard is also useful for manual testing preparations. Additional features like custom domains, webhooks, auto-reply and forwarding are worth checking out for more complex scenarios.</p>"
Metabase,Contain,Tools,TRUE,new,"<p><strong><a href=""https://metabase.com/"">Metabase</a></strong> is an open-source analytics and business intelligence tool that allows users to visualize and analyze data from a variety of data sources, including relational and NoSQL databases. The tool helps users create visualizations and reports, organize them into dashboards and easily share insights. It also offers an SDK for embedding interactive dashboards in web applications, matching the theme and style of the application — making it developer-friendly. With both officially supported and community-backed data connectors, Metabase is versatile across data environments. As a lightweight BI tool, our teams find it useful for managing interactive dashboards and reports in their applications.</p>"
NeMo Guardrails,Contain,Tools,FALSE,moved in,"<p><strong><a href=""https://github.com/NVIDIA/NeMo-Guardrails"">NeMo Guardrails</a></strong> is an easy-to-use open-source toolkit from NVIDIA that empowers developers to implement guardrails for LLMs used in conversational applications. Since we last mentioned it in the Radar, NeMo has seen significant adoption across our teams and continues to improve. Many of the latest enhancements to NeMo Guardrails focus on expanding integrations and strengthening security, data and control, aligning with the project’s core goal.</p>

<p>A major update to NeMo’s <a href=""https://docs.nvidia.com/nemo/guardrails/latest/index.html"">documentation</a> has improved usability and new integrations have been added, including <a href=""https://www.autoalign.ai/"">AutoAlign</a> and <a href=""https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model"">Patronus Lynx</a>, along with support for Colang 2.0. Key upgrades include enhancements to <a href=""https://github.com/NVIDIA/NeMo-Guardrails/releases/tag/v0.11.1"">content safety and security</a> as well as a recent release that supports streaming LLM content through output rails for improved performance. We've also seen added support for <a href=""https://docs.nvidia.com/nemo/guardrails/latest/user-guides/community/prompt-security.html"">Prompt Security</a>. Additionally, Nvidia <a href=""https://blogs.nvidia.com/blog/nemo-guardrails-nim-microservices/"">released</a> three new microservices: <a href=""https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety"">content safety NIM microservice</a>, <a href=""https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-topic-control"">topic control NIM microservice</a> and <a href=""https://build.nvidia.com/nvidia/nemoguard-jailbreak-detect"">jailbreak detection</a>, all of which have been integrated with NeMo Guardrails.</p>

<p>Based on its growing feature set and increased usage in production, we’re moving NeMo Guardrails to Trial. We recommend reviewing the latest <a href=""https://github.com/NVIDIA/NeMo-Guardrails/releases"">release notes</a> for a complete overview of the changes since our last blip.</p>"
Nyx,Contain,Tools,TRUE,new,"<p><strong><a href=""https://github.com/mooltiverse/nyx"">Nyx</a></strong> is a versatile semantic release tool that supports a wide range of software engineering projects. It’s language-agnostic and works with all major CI and SCM platforms, making it highly adaptable. While many teams use semantic versioning in <a href=""https://www.thoughtworks.com/en-us/insights/blog/enabling-trunk-based-development-deployment-pipelines"">trunk-based development</a>, Nyx also supports workflows like <a href=""/radar/techniques/gitflow"">Gitflow</a>, OneFlow and GitHub Flow. One key advantage of Nyx in production is its automatic changelog generation, with built-in support for <a href=""https://www.conventionalcommits.org/en/v1.0.0/"">Conventional Commits</a>.</p>

<p>As noted in previous Radar editions, we caution against development patterns that rely on <a href=""/radar/techniques/long-lived-branches-with-gitflow"">long-lived branches</a> (e.g., <a href=""/radar/techniques/gitflow"">Gitflow</a>, <a href=""/radar/techniques/gitops"">GitOps</a>), as they introduce challenges that even powerful tools like Nyx cannot mitigate. We highly recommend trying Nyx in CI/CD workflows, especially for trunk-based development, where we’ve seen repeated success.</p>"
OpenRewrite,Contain,Tools,FALSE,moved in,"<p><strong><a href=""https://github.com/openrewrite/rewrite"">OpenRewrite</a></strong> continues to serve us well as a tool for large-scale refactorings that follow a set of rules such as moving to a new API version of a widely used library or applying updates to many services that were created from the same template. Support for languages beyond Java, notably JavaScript, has been introduced. With short LTS release cycles in frameworks like Angular, keeping projects updated to newer versions has become increasingly important. OpenRewrite supports this process effectively. Using an AI coding assistant is an alternative, but for rule-based changes, it’s usually slower, more expensive and less reliable. We like that OpenRewrite comes bundled with a catalog of recipes (rules), which describe the changes to be made. The refactoring engine, bundled recipes and build tool plugins are open-source software, which makes it easier for teams to reach for OpenRewrite when they need it.</p>"
Plerion,Contain,Tools,TRUE,new,"<p><strong><a href=""https://www.plerion.com/"">Plerion</a></strong> is an AWS-focused cloud security platform that integrates with hosting providers to uncover risks, misconfigurations and vulnerabilities across your cloud infrastructure, servers and applications. Similar to <a href=""/radar/tools/wiz"">Wiz</a>, Plerion uses risk-based prioritization for detected issues, promising to let you ""focus on the 1% that matters."" Our teams report positive experiences with Plerion, noting it has provided our clients with significant insights and reinforced the importance of proactive security monitoring for their organizations.</p>"
Software engineering agents,Contain,Tools,FALSE,moved in,"<p>Since we last wrote about <strong>software engineering agents</strong> six months ago, the industry still lacks a shared definition of the term ""agent."" However, a major development has emerged — not in fully autonomous coding agents (which remain unconvincing) but in supervised agentic modes within the IDE. These modes allow developers to drive implementation via chat, with tools not only modifying code in multiple files but also executing commands, running tests and responding to IDE feedback like linting or compile errors.</p>

<p>This approach, sometimes called chat-oriented programming (CHOP) or prompt-to-code, keeps developers in control while shifting more responsibility to AI than traditional coding assistants like auto-suggestions. Leading tools in this space include <a href=""/radar/tools/cursor"">Cursor</a>, <a href=""/radar/tools/cline"">Cline</a> and <a href=""/radar/tools/windsurf"">Windsurf</a>, with <a href=""/radar/tools/github-copilot"">GitHub Copilot</a> slightly behind but catching up. The usefulness of these agentic modes depends on both the model used (with <a href=""/radar/tools/claude-sonnet"">Claude's Sonnet series</a> the current state of the art) and how well the tool integrates with the IDE to provide a good developer experience.</p>

<p>We've found these workflows intriguing and promising, with a notable increase in coding speed. However, keeping problem scopes small helps developers better review AI-generated changes. This works best with low-abstraction prompts and <a href=""/radar/techniques/ai-friendly-code-design"">AI-friendly codebases</a> that are well-structured and properly tested. As these modes improve, they’ll also heighten the risk of <a href=""/radar/techniques/complacency-with-ai-generated-code"">complacency with AI-generated code</a>. To mitigate this, employ pair programming and other disciplined review practices, especially for production code.</p>"
Tuple,Contain,Tools,FALSE,no change,"<p><strong><a href=""https://tuple.app/"">Tuple</a></strong>, a tool optimized for remote pair programming, was originally designed to fill the gap left by Slack’s Screenhero. Since we last mentioned it in the Radar, it has seen wider adoption, addressed previous quirks and constraints and now supports Windows. A key improvement is enhanced desktop sharing with a built-in privacy feature, allowing users to hide private app windows (such as text messages) while sharing tools like the browser window. Previously, UI limitations made Tuple feel like a pair programming tool rather than a general collaboration tool. With these updates, users can now collaborate on content beyond the IDE.</p>

<p>However, it’s important to note that the remote pair has access to the entire desktop. If not configured properly, this could be a security concern, especially if the pair is not trustworthy. We strongly recommend educating teams on Tuple’s privacy settings, best practices and etiquette before use.</p>

<p>We encourage teams to try the latest version of Tuple in your development workflow. It aligns with our <a href=""/radar/techniques/pragmatic-remote-pairing"">pragmatic remote pairing</a> recommendation, offering low-latency pairing, an intuitive UX and significant usability improvements.</p>"
Turborepo,Contain,Tools,FALSE,moved in,"<p><strong><a href=""https://turbo.build/repo"">Turborepo</a></strong> helps manage large JavaScript or TypeScript monorepos by analyzing, caching, parallelizing and optimizing build tasks to speed up the process. In large monorepos, projects often depend on each other; rebuilding all dependencies for every change is inefficient and time-consuming, but Turborepo makes this easier. Unlike <a href=""/radar/tools/nx"">Nx</a>, Turborepo's default setup uses multiple package.json files — one per project — which allows having dependencies with different versions (multiple versions of React, for example) in a single monorepo, which Nx discourages. While this might be considered an anti-pattern, it does address certain use cases, like migrating from multi- to monorepo, where teams may temporarily require multiple versions of dependencies. In our experience, TurboRepo is quite simple to set up and performs well.</p>"
AnythingLLM,Assess,Tools,TRUE,new,"<p><strong><a href=""https://github.com/Mintplex-Labs/anything-llm"">AnythingLLM</a></strong> is an open-source desktop application to chat with large documents or pieces of content, backed by out-of-the-box integration with LLMs and vector databases. It has a pluggable architecture for embedder models and can be used with most of the commercial LLMs as well as open-weight models that can be managed by <a href=""/radar/tools/ollama"">Ollama</a>. In addition to <a href=""/radar/techniques/retrieval-augmented-generation-rag"">RAG</a>, different skills can be created and organized as agents to perform custom tasks and workflows. It lets users organize the documents and interactions with them in different workspaces and they act as long lived threads with different contexts. Recently, it also became possible to deploy it as a multi-user web application with a simple Docker image. Some of our teams are using it as a local personal assistant and finding it a powerful and useful utility.</p>"
Gemma Scope,Assess,Tools,TRUE,new,"<p>Mechanistic interpretability — understanding the inner workings of large language models — is becoming an increasingly important field. Tools like <strong><a href=""https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/"">Gemma Scope</a></strong> and the open-source library <a href=""https://github.com/google-deepmind/mishax"">Mishax</a> provide insights into the Gemma2 family of open models. Interpretability tools play a crucial role in debugging unexpected behavior, identifying components responsible for hallucinations, biases or other failure cases, and ultimately building trust by offering deeper visibility into models. While this field may be of particular interest to researchers, it's worth noting that with the recent release of <a href=""/radar/platforms/deepseek-r1"">DeepSeek-R1</a>, model training is becoming more feasible for companies beyond the established players. As GenAI continues to evolve, both interpretability and safety will only grow in importance.</p>"
Hurl,Assess,Tools,TRUE,new,"<p><strong><a href=""https://hurl.dev/"">Hurl</a></strong> is a Swiss Army knife for making sequences of HTTP requests, defined in plain text files using Hurl-specific syntax. Beyond sending requests, Hurl can validate responses, ensuring a request returns a specific HTTP status code; assert conditions on response headers or content using XPATH, JSONPath or regular expressions; and extract response data into variables, which can then be used to chain requests.</p>

<p>With its feature set, Hurl is useful for simple API automations but also serves as an automated API testing tool. Its ability to generate detailed test reports in HTML or JSON enhances its utility for testing workflows. While dedicated tools like Bruno and Postman offer GUIs and additional features, we like Hurl for its simplicity. Like <a href=""/radar/tools/bruno"">Bruno</a>, which also uses plain text files, Hurl tests can be stored in the code repository.</p>"
Jujutsu,Assess,Tools,TRUE,new,"<p><a href=""/radar/tools/git"">Git</a> is the dominant distributed version control system (VCS), holding the vast majority of market share. Yet, despite over a decade of dominance, developers still struggle with its complex workflows for branching, merging, rebasing and conflict resolution. This ongoing frustration has fueled a wave of tools designed to ease the pain — some offering visualizations to clarify complexity, others providing their own graphical interfaces to abstract it away entirely.</p>

<p><strong><a href=""https://github.com/jj-vcs/jj"">Jujutsu</a></strong> takes this a step further, offering a full-fledged alternative to Git while maintaining compatibility by <a href=""https://jj-vcs.github.io/jj/latest/git-compatibility/"">using Git repositories as a storage backend</a>. This allows developers to utilise existing Git servers and services while benefiting from Jujutsu's streamlined workflows. Positioned as ""both simple and powerful,"" Jujutsu emphasizes ease of use for developers of all experience levels. One standout feature is its <a href=""https://jj-vcs.github.io/jj/latest/conflicts/"">first-class conflict resolution</a>, which has the potential to significantly improve the developer experience.</p>"
kubenetmon,Assess,Tools,TRUE,new,"<p>Monitoring and understanding the network traffic associated with Kubernetes can prove a challenge, particularly when your infrastructure spans multiple zones, regions or clouds. <strong><a href=""https://clickhouse.com/blog/kubenetmon-open-sourced"">kubenetmon</a></strong>, built by <a href=""/radar/tools/clickhouse"">ClickHouse</a> and recently open sourced, hopes to solve this problem by offering detailed Kubernetes data transfer metering across the major cloud providers. If you're running Kubernetes and have been frustrated by opaque data transfer costs on your bill it may be worth exploring kubenetmon.</p>"
Mergiraf,Assess,Tools,TRUE,new,"<p>Resolving merge conflicts is probably one of the least liked activities in software development. And while there are techniques that reduce the complexity of merges — for example, practicing continuous integration in the original sense of merging to a shared mainline at least daily — we're seeing too much effort spent on merges. <a href=""/radar/techniques/long-lived-branches-with-gitflow"">Long-lived feature branches</a> are one culprit, but AI-assisted coding also has a tendency to increase the size of change sets. Help may come in the form of <strong><a href=""https://mergiraf.org/"">Mergiraf</a></strong>, a new tool that resolves merge conflicts by looking at the syntax tree rather than treating code as lines of text. As a git merge driver, it can be set up so that git subcommands like <code>merge</code> and <code>cherry-pick</code> automatically use Mergiraf instead of the default heuristics.</p>"
ModernBERT,Assess,Tools,TRUE,new,"<p>The successor to <a href=""/radar/techniques/bert"">BERT</a> (Bidirectional Encoder Representations from Transformers), <strong><a href=""https://huggingface.co/blog/modernbert"">ModernBERT</a></strong> is a next-generation family of encoder-only transformer models designed for a wide range of natural language processing (NLP) tasks. As a drop-in replacement, ModernBERT improves both performance and accuracy while addressing some of BERT's limitations — notably including support for dramatically longer context lengths thanks to Alternating Attention. Teams with NLP needs should consider ModernBERT before defaulting to a <a href=""/radar/techniques/overenthusiastic-llm-use"">general-purpose generative model</a>.</p>"
OpenRouter,Assess,Tools,TRUE,new,"<p><strong><a href=""https://openrouter.ai/"">OpenRouter</a></strong> is a unified API for accessing multiple large language models. It provides a single integration point for <a href=""https://openrouter.ai/models"">mainstream LLM providers</a>, simplifies experimentation, reduces vendor lock-in, and optimizes costs by routing requests to the most appropriate model. Popular tools like <a href=""/radar/tools/cline"">Cline</a> and <a href=""/radar/platforms/open-webui"">Open WebUI</a> use OpenRouter as their endpoint. During our Radar discussion, we questioned whether most projects truly need to switch between models, given that OpenRouter must add price markup as a profit model on top of this encapsulation layer. However, we also recognize that OpenRouter provides various load-balancing strategies to help optimize costs. One particularly useful feature is its ability to bypass API rate limits. If your application exceeds the rate limit of a single LLM provider, OpenRouter can help you break through this limitation and achieve better throughput.</p>"
Redactive,Assess,Tools,TRUE,new,"<p><strong><a href=""https://www.redactive.ai/"">Redactive</a></strong> is an enterprise AI enablement platform designed to help regulated organizations securely prepare unstructured data for AI applications, such as AI-powered assistants and copilots. It integrates with content platforms like Confluence, creating secure text indices for <a href=""/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> searches. By serving only live data and enforcing real-time user permissions from source systems, Redactive ensures AI models access accurate, authorized information without compromising security. Additionally, it provides engineering teams with tools to build AI use cases safely using any LLM. For organizations exploring AI-driven solutions, Redactive offers a streamlined approach to data preparation and compliance, balancing security and accessibility for teams experimenting with AI capabilities in a controlled environment.</p>"
System Initiative,Assess,Tools,FALSE,no change,"<p>We continue to be excited by <strong><a href=""https://www.systeminit.com/"">System Initiative</a></strong>. This experimental tool represents a radical new direction for DevOps work. We really like the creative thinking that has gone into this tool and hope it will encourage others to break with the status quo of infrastructure-as-code approaches. System Initiative is now out of beta and available free and open source under an Apache 2.0 license. While the tool’s developers use it to manage production infrastructure, it still has a way to go before it can scale to meet the demands of large enterprises. However, we continue to think it's worth checking out to experience a completely different approach to DevOps tooling.</p>"
TabPFN,Assess,Tools,TRUE,new,"<p><strong><a href=""https://github.com/PriorLabs/TabPFN"">TabPFN</a></strong> is a transformer-based model designed for fast and accurate classification on small tabular data sets. It leverages in-context learning (ICL) to make predictions directly from labeled examples without hyperparameter tuning or additional training. Pretrained on millions of synthetic data sets, TabPFN generalizes well across diverse data distributions and handles missing values and outliers effectively. Its strengths include efficient processing of heterogeneous data and robustness to uninformative features.</p>

<p>TabPFN is particularly suitable for small-scale applications where speed and accuracy are crucial. However, it faces scalability challenges with larger data sets and has limitations in handling regression tasks. As a cutting-edge solution, TabPFN is worth evaluating for its potential to outperform traditional models in tabular classification, especially where transformers are less commonly applied.</p>"
v0,Assess,Tools,TRUE,new,"<p><strong><a href=""https://v0.dev/"">v0</a></strong> by Vercel is an AI tool for generating front-end code from a screenshot, Figma design or simple prompt. It supports <a href=""/radar/Solutions/react-js"">React</a>, <a href=""/radar/Solutions/vue-js"">Vue</a>, <a href=""/radar/Solutions/shadcn"">shadcn</a> and <a href=""/radar/Solutions/tailwind-css"">Tailwind</a> among other front-end frameworks. Beyond AI-generated code, v0 offers a great user experience, including the ability to preview the generated code and deploy it to Vercel in one step. While building real-world applications involves integrating multiple functionalities beyond a single screen, v0 provides a solid way to prototype and can be used to initialize a starting point for developing complex applications.</p>"
Windsurf,Assess,Tools,TRUE,new,"<p><strong><a href=""https://codeium.com/windsurf"">Windsurf</a></strong> is an AI coding assistant by Codeium that stands out for its agentic capabilities. Similar to <a href=""/radar/tools/cursor"">Cursor</a> and <a href=""/radar/tools/cline"">Cline</a>, it lets developers drive their implementation from an AI chat that navigates and changes code and executes commands. It frequently releases interesting new features and integrations for the agentic mode. Recently, for instance, it released a browser preview that makes it easy for the agent to access DOM elements and the browser console, and a web research capability that lets Windsurf look for documentation and solutions on the internet when appropriate. Windsurf provides access to a range of popular models, and users can activate and reference web search, library documentation and MCP integration as additional context providers.</p>"
YOLO,Assess,Tools,TRUE,new,"<p>The <strong><a href=""https://docs.ultralytics.com/models/yolo11/"">YOLO</a></strong> (You Only Look Once) series, developed by Ultralytics, continues to advance computer vision models. The latest release, YOLO11, delivers significant improvements in both precision and efficiency over previous versions. YOLO11 can perform image classification at <a href=""https://docs.ultralytics.com/models/yolo11/#performance-metrics"">high speed with minimum resources</a>, making it suitable for real-time applications in edge devices. We also found that the ability to use the same framework to do pose estimation, object detection, image segmentation and other tasks is very powerful. This significant development also reminds us that using ‘traditional’ machine-learning models for specific tasks can be more powerful than general AI models, such as LLMs.</p>"
OpenTelemetry,Adopt,Solutions,FALSE,moved in,"<p><strong><a href=""https://opentelemetry.io"">OpenTelemetry</a></strong> is quickly becoming the industry standard for observability. The release of the <a href=""https://opentelemetry.io/docs/specs/otel/protocol/"">OpenTelemetry protocol (OTLP)</a> specification established a standardized way to handle traces, metrics and logs, reducing the need for multiple integrations or major rewrites as monitoring distributed solutions and interoperability requirements grow. As OpenTelemetry expands to support logs and profiling, OTLP ensures a consistent transport format across all telemetry data, simplifying instrumentation and making full-stack observability more accessible and scalable for microservices architectures.</p>

<p>Adopted by vendors like <a href=""https://docs.datadoghq.com/es/opentelemetry/interoperability/otlp_ingest_in_the_agent/?tab=host"">Datadog</a>, <a href=""https://docs.newrelic.com/docs/opentelemetry/get-started/collector-processing/opentelemetry-collector-processing-intro/"">New Relic</a> and <a href=""https://grafana.com/docs/grafana-cloud/send-data/otlp/"">Grafana</a>, OTLP enables organizations to build flexible, vendor-agnostic observability stacks without being locked into proprietary solutions. It supports gzip and zstd compression, reducing telemetry data size and lowering bandwidth usage — a key advantage for environments handling high volumes of telemetry data. Designed for long-term growth, OTLP ensures OpenTelemetry remains a robust and future-proof standard, solidifying its position as the de-facto choice for telemetry transport.</p>"
React Hook Form,Adopt,Solutions,FALSE,moved in,"<p>We noted <strong><a href=""/radar/Solutions/react-hook-form"">React Hook Form</a></strong> as an alternative to Formik. By defaulting to uncontrolled components, it delivers significantly better out-of-the-box performance, especially for large forms. React Hook Form is well-integrated with various schema-based validation libraries, including <a href=""https://github.com/jquense/yup"">Yup</a>, <a href=""https://github.com/colinhacks/zod"">Zod</a> and more. Additionally, React Hook Form offers a lot of flexibility, making it easy to integrate with existing codebases and other libraries. You can use React Hook Form with external controlled components libraries such as <a href=""/radar/Solutions/shadcn"">shadcn</a> or <a href=""https://ant.design/"">AntD</a>. With strong performance, seamless integration and active development, it’s a solid choice for building large form or form-heavy application.</p>"
Effect,Contain,Solutions,TRUE,new,"<p><strong><a href=""https://effect.website/"">Effect</a></strong> is a powerful <a href=""/radar/Solutions/typescript"">TypeScript</a> library for building complex synchronous and asynchronous programs. Web application development often requires boilerplate code for tasks such as asynchrony, concurrency, state management and error handling. Effect-TS streamlines these processes using a functional programming approach. Leveraging TypeScript’s type system, Effect helps catch hard-to-detect issues at compile time. Our team previously used <a href=""https://github.com/gcanti/fp-ts"">fp-ts</a> for functional programming but found that Effect-TS provides abstractions that align more closely with daily tasks. It also makes code easier to combine and test. While traditional approaches like <code>Promise/try-catch</code> or <code>async/await</code> can handle such scenarios, after using Effect, our team found no reason to go back.</p>"
Hasura GraphQL engine,Contain,Solutions,TRUE,new,"<p>The <strong><a href=""https://github.com/hasura/graphql-engine"">Hasura GraphQL engine</a></strong> is a universal data access layer that simplifies building, running and governing high-quality APIs on different data sources. It provides instant <a href=""/radar/Solutions/graphql"">GraphQL</a> APIs over various databases (including PostgreSQL, MongoDB and ClickHouse) and data sources, enabling developers to fetch only the data they need quickly and securely. We found Hasura easy to implement GraphQL for <a href=""/radar/techniques/graphql-for-server-side-resource-aggregation"">server-side resource aggregation</a> and have applied it in multiple <a href=""/radar/techniques/graphql-for-data-products"">data product projects</a>. However, we <a href=""/radar/techniques/apollo-federation"">remain cautious</a> about its powerful federated query and unified schema management. A noteworthy recent addition is Hasura's <a href=""https://promptql.hasura.io/"">PromptQL</a> feature, which allows developers to leverage LLMs for more natural and intuitive data interactions.</p>"
LangGraph,Contain,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/langchain-ai/langgraph"">LangGraph</a></strong> is an orchestration framework designed to build stateful <a href=""/radar/techniques/llm-powered-autonomous-agents"">multi-agent applications</a> using LLMs. It provides a lower-level set of primitives like edges and nodes compared to <a href=""/radar/Solutions/langchain"">LangChain</a>’s higher-level abstractions, offering developers fine-grained control over agent workflows, memory management and state persistence. This graph-based approach ensures predictable and customizable workflows, making debugging, scaling and maintaining production applications easier. Although it has a steeper learning curve, LangGraph's lightweight design and modularity make it a powerful framework for creating agentic applications.</p>"
MarkItDown,Contain,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/microsoft/markitdown"">MarkItDown</a></strong> converts various formats (PDF, HTML, PowerPoint, Word) into Markdown, enhancing text readability and context retention. Since LLMs derive context from <a href=""https://towardsdatascience.com/making-text-data-ai-ready-81d7fa83fd58/"">formatting cues</a> like headings and sections, Markdown helps preserve structure for better comprehension. In <a href=""/radar/techniques/retrieval-augmented-generation-rag"">RAG</a>-based applications, our teams used MarkItDown to pre-process documents into Markdown, ensuring logical markers (headers, subsections) remained intact. Before embedding generation, structure-aware chunking helped maintain full section context which improves the clarity of query responses, especially for complex documents. Widely used for documentation, Markdown also makes MarkItDown’s CLI a valuable developer productivity tool.</p>"
Module Federation,Contain,Solutions,FALSE,moved in,"<p><strong><a href=""https://module-federation.io/guide/start/index.html"">Module Federation</a></strong> allows for the specification of shared modules and dependency deduplication across micro frontends. With version 2.0, it has evolved to function independently of webpack. This update introduces key features, including a federation run time, a new plugin API and support for popular frameworks like React and Angular as well as popular bundlers like <a href=""/radar/tools/rspack"">Rspack</a> and <a href=""/radar/tools/vite"">Vite</a>. By adopting Module Federation, large web applications can be divided into smaller, manageable micro frontends, allowing different teams to develop, deploy and scale independently while sharing dependencies and components efficiently.</p>"
Prisma ORM,Contain,Solutions,TRUE,new,"<p><strong><a href=""https://www.prisma.io/orm"">Prisma ORM</a></strong> is an open-source database toolkit that simplifies working with databases in Node.js and TypeScript applications. It offers a modern, type-safe approach to database access, automates database schema migrations and provides an intuitive query API. Unlike typical ORMs, PrismaORM uses plain JavaScript objects to define database types without decorators or classes. Our experience with Prisma ORM is positive; we find it not only better aligns with the general TypeScript development landscape, it also neatly integrates with the functional programming paradigm.</p>"
.NET Aspire,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://learn.microsoft.com/en-us/dotnet/aspire/get-started/aspire-overview"">.NET Aspire</a></strong> is designed to simplify the orchestration of distributed applications on a developer's local machine. Aspire lets you orchestrate multiple services in a local development environment — including multiple .NET projects, dependent databases and Docker containers — all with a single command. Furthermore, Aspire provides observability tools — including logging, tracing and metrics dashboards — for local development, decoupled from the tools used in staging or production environments. This significantly improves the developer experience when building, tweaking and debugging the observability aspects of any system they are working on.</p>"
Android XR SDK,Assess,Solutions,TRUE,new,"<p>Google, in collaboration with Samsung and Qualcomm, has introduced Android XR, a new operating system designed for XR headsets. Support is planned for glasses and other devices. Most Android apps are supported with no or minimal changes, but the idea is to build new spatial apps from scratch or to ""spatialize"" existing apps. The new <strong><a href=""https://developer.android.com/develop/xr"">Android XR SDK</a></strong> is positioned as the go-to SDK for such projects, and Google provides <a href=""https://developer.android.com/develop/xr/get-started#select-development"">guidance</a> on how to choose tools and technologies bundled in the SDK. It’s available in developer preview now.</p>"
Browser Use,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/browser-use/browser-use"">Browser Use</a></strong> is an open-source python library that enables LLM-based AI agents to use web browsers and access web applications. It can control the browser and perform steps that include navigations, inputs and text extractions. With the ability to manage multiple tabs, it can perform coordinated actions across multiple web apps. It’s useful for scenarios where LLM-based agents need to access web content, perform actions on it and get the results. The library can work with a variety of LLMs. It leverages <a href=""/radar/Solutions/playwright"">Playwright</a> to control the browser, combining visual understanding with HTML structure extraction for improved web interaction. This library is gaining traction in multi-agent scenarios, enabling agents to collaborate on complex workflows involving web interactions.</p>"
CrewAI,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/crewAIInc/crewAI"">CrewAI</a></strong> is a platform designed to help you build and manage AI agents that can work together to accomplish complex tasks. Think of it as a way to create a crew of AI workers, each with their own special skills, who can collaborate to achieve a common goal. We’ve mentioned it previously in the Radar under <a href=""/radar/techniques/llm-powered-autonomous-agents"">LLM-powered autonomous agents</a>. In addition to the open-source Python library, CrewAI now has an enterprise solution so organizations can create agent-based applications for real-world business cases, run them on their cloud infrastructure and connect to existing data sources such as Sharepoint or JIRA. We've used CrewAI multiple times to tackle production challenges, from automated validation of promotion codes to investigating transaction failures and customer support queries. While the agentic landscape continues to evolve rapidly, we’re confident in placing CrewAI in Assess.</p>"
ElysiaJs,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://elysiajs.com/"">ElysiaJS</a></strong> is an end-to-end type-safe web framework for TypeScript, designed primarily for <a href=""/radar/platforms/bun"">Bun</a> but also compatible with other JavaScript run times. Unlike alternatives such as <a href=""https://trpc.io/"">tRPC</a>, which enforces specific API interface structures, ElysiaJS does not impose any API interface structure. This allows developers to create APIs that follow established industry practices such as RESTful, JSON:API or OpenAPI and also provide end-to-end type safety. ElysiaJS is highly performant when used with Bun run time, even comparable to Java or Go web frameworks in some benchmarks. ElysiaJS is worth considering, especially when building a <a href=""/radar/techniques/bff-backend-for-frontends"">backend-for-frontend (BFF)</a>.</p>"
FastGraphRAG,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/circlemind-ai/fast-graphrag"">FastGraphRAG</a></strong> is an open-source implementation of <a href=""/radar/techniques/graphrag"">GraphRAG</a> designed for high retrieval accuracy and performance. It uses <a href=""https://arxiv.org/abs/2006.11876"">Personalized PageRank</a> to limit graph navigation to the most relevant nodes among all the related nodes in the graph, enhancing retrieval accuracy and improving LLM response quality. It also provides a visual representation of the graph, helping users understand node relationships and the retrieval process. With support for incremental updates, it’s well-suited to dynamic and evolving data sets. Optimized for large-scale GraphRAG use cases, FastGraphRAG improves performance while minimizing resource consumption.</p>"
Gleam,Assess,Solutions,TRUE,new,"<p><a href=""https://www.erlang.org/"">Erlang/OTP</a> is a powerful platform for building highly concurrent, scalable and fault-tolerant distributed systems. Traditionally, its languages have been dynamically typed, but <strong><a href=""https://gleam.run/"">Gleam</a></strong> introduces type safety at the language level. Built on <a href=""https://en.wikipedia.org/wiki/BEAM_(Erlang_virtual_machine)"">BEAM</a>, Gleam combines the expressiveness of functional programming with compile-time type safety, reducing run-time errors and improving maintainability. With a modern syntax, it integrates well with the OTP ecosystem, leveraging the strengths of Erlang and <a href=""/radar/Solutions/elixir"">Elixir</a> while ensuring strong interoperability. The Gleam community is active and welcoming, and we look forward to its continued development.</p>"
GoFr,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/gofr-dev/gofr"">GoFr</a></strong> is a framework for building microservices in <a href=""/radar/Solutions/go-language"">Golang</a>, designed to simplify development by abstracting away boilerplate code for common microservice functionalities such as logging, traces, metrics, configuration management and Swagger API documentation. It supports multiple databases, handles database migrations and facilitates pub/sub with brokers like Kafka and NATs. Additionally, GoFr includes task scheduling with cron jobs. It reduces the complexity of building and maintaining microservices, and allows developers to focus on writing business logic rather than infrastructure concerns. Even though there are other popular Go libraries for building web APIs, GoFr is gaining traction and is worth exploring for Golang-based microservices.</p>"
Java post-quantum cryptography,Assess,Solutions,TRUE,new,"<p>At the core of asymmetric cryptography, which secures most modern communication, lies a mathematically hard problem. However, the problem used in today's algorithms will be easy to solve with quantum computers, driving research for alternatives. <a href=""https://en.wikipedia.org/wiki/Lattice-based_cryptography"">Lattice-based cryptography</a> is currently the most promising candidate. Although cryptographically relevant quantum computers are still years away, post-quantum cryptography is worth considering for applications that must remain secure for decades. There is also the risk that encrypted data is recorded today in order to be decrypted once quantum computers become available.</p>

<p><strong>Java post-quantum cryptography</strong> takes its first steps in <a href=""https://openjdk.org/projects/jdk/24/"">JDK 24</a>, set for general availability in late March. This release includes <a href=""https://openjdk.org/jeps/496"">JEP 496</a> and <a href=""https://openjdk.org/jeps/497"">JEP 497</a> — which implement a key encapsulation mechanism and a digital signature algorithm — both standards-based and designed to be resistant to future quantum computing attacks. While <a href=""https://openquantumsafe.org/liboqs/"">liboqs</a> from the Open Quantum Safe project provides C-based implementations with a JNI wrapper, it’s encouraging to see a native Java implementation emerging as well.</p>"
Presidio,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/microsoft/presidio"">Presidio</a></strong> is a data protection SDK for <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#analyze-text-for-pii-entities"">identifying</a> and <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#anonymize-text-with-identified-pii-entities"">anonymizing</a> sensitive data in structured and unstructured text. It detects personally identifiable information (PII) such as credit card numbers, names and locations, using named entity recognition, regular expressions and rule-based logic. Presidio supports <a href=""https://microsoft.github.io/presidio/samples/python/presidio_notebook/#create-custom-pii-entity-recognizers"">custom</a> PII entity recognition and de-identification, allowing businesses to tailor it to their specific privacy requirements. Although Presidio automates the identification of sensitive information, it’s not foolproof and may miss or misidentify data. Exercise caution when relying on its results.</p>"
PydanticAI,Assess,Solutions,TRUE,new,"<p>As the technologies to build LLM-backed applications and agents continue to evolve rapidly, frameworks for building and orchestrating such applications often struggle to keep up or find the right timeless abstractions. <strong><a href=""https://ai.pydantic.dev/"">PydanticAI</a></strong> is the latest entrant in this space, aiming to simplify implementations while avoiding unnecessary complexity. Developed by the creators of the popular <a href=""/radar/Solutions/pydantic"">Pydantic</a>, it builds on lessons learned from earlier frameworks — many of which already rely on Pydantic. Rather than trying to be a Swiss Army knife, PydanticAI offers a lightweight yet powerful approach. It integrates with all major model APIs, includes built-in <a href=""/radar/techniques/structured-output-from-llms"">structured output handling</a> and introduces a graph-based abstraction for managing complex agentic workflows.</p>"
Swift for resource-constrained applications,Assess,Solutions,TRUE,new,"<p>Since the release of <a href=""https://www.swift.org/blog/announcing-swift-6/"">Swift 6.0</a>, the language has expanded beyond Apple's ecosystem with improved support for major operating systems, making it more viable to use <strong>Swift for resource-constrained applications</strong>. Traditionally, this space has been dominated by C, C++ and, more recently, Rust, due to their low-level control, high performance and availability of certified compilers and libraries that comply with standards such as MISRA, ISO 26262 and ASIL. While Rust has begun achieving similar certifications, Swift has yet to pursue this process, limiting its use in safety-critical applications.</p>

<p>Swift's growing adoption is driven by its balance of performance and safety features, including strong type safety and automatic reference counting for memory management. While Rust’s ownership model offers stronger memory safety guarantees, Swift provides a different trade-off that some developers find more approachable. Both Swift and Rust share the LLVM/Clang compiler backend, allowing advancements in one to benefit the other. With its ability to compile to optimized machine code, its open-source development and its expanding cross-platform support, Swift is emerging as a contender for a wider range of applications — far beyond its iOS roots.</p>"
Tamagui,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/tamagui/tamagui"">Tamagui</a></strong> is a library for efficiently sharing styles between React web and React Native. It offers a <a href=""https://tamagui.dev/ui/intro"">design system</a> with reusable styled and unstyled components that render seamlessly across platforms. Its optional <a href=""https://tamagui.dev/docs/intro/compiler-install"">optimizing compiler</a> boosts performance by converting styled components into atomic CSS with divs on the web and hoisted style objects on native views.</p>"
torchtune,Assess,Solutions,TRUE,new,"<p><strong><a href=""https://github.com/pytorch/torchtune"">torchtune</a></strong> is a <a href=""/radar/Solutions/pytorch"">PyTorch</a> library for authoring, post-training and experimenting with LLMs. It supports single and multi-GPU setups and enables distributed training with <a href=""https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md"">FSDP2</a>. The library provides YAML-based <a href=""https://pytorch.org/torchtune/main/recipes/recipes_overview.html"">recipes</a> for tasks like fine-tuning, inference, evaluation and quantization-aware training. Each recipe offers a focused set of features, avoiding complex flag-based configurations. It prioritizes simplicity, favoring code clarity over excessive abstractions. It also includes a <a href=""https://pytorch.org/torchtune/main/tune_cli.html"">CLI</a> for downloading models, managing recipes and running experiments efficiently.</p>"
Node overload,End-of-Life,Solutions,FALSE,no change,"<p>A few years ago, we observed <strong>Node overload</strong>: Node.js was often used for questionable reasons or without even considering any alternatives. While we understand that some teams prefer a single-language stack — despite the trade-offs — we continue to advocate for <a href=""/radar/techniques/polyglot-programming"">polyglot programming</a>. At the time, we noted that Node.js had a deserved reputation for efficiency in IO-heavy workloads, but we mentioned that other frameworks had caught up which offered better APIs and superior overall performance. We also cautioned that Node.js was never well-suited to compute-heavy workloads, a limitation that remains a significant challenge. Now, with the rise of data-heavy workloads, we’re seeing teams struggle with these as well.</p>"
